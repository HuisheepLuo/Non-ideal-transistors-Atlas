import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt
import pandas as pd


device = torch.device("cpu")
cpu = torch.device("cpu")

def baseline_value(dir, idx, column_begin=0):
    '''
    Function for read baseline from csv.
    '''
    df = pd.read_csv(dir)
    column_end = column_begin + 200
    y = torch.tensor(df.iloc[idx, column_begin:column_end].astype(np.float32))
    return y

def integrated_gradients(inputs, target_labels, model, loss_fn, optimizer, predict_and_gradients, baseline, steps=50):
    '''
    Using integrated gradients method to attribute the inputs of the network.

    Args:
        inputs[np.ndarray]: input `X` of dataset.
        target_labels[np.ndarray]: labels of dataset.
        model[nn.Module]: network model.
        loss_fn: used loss function for training.
        optimizer: used optimizer for training.
        predict_and_gradients: the import function `pred_and_grad`.
        baseline[np.ndarray]: the baseline input `X'`, should be as same length as `X`.
        step[int]: the step of interpolation.

    Returns:
        integrated_grad[np.ndarray]: the attribution value generated by Integrated Gradient method.
        scaled_input[list(np.ndarray)]: the interpolation from `X` to `X'` .
        
    '''
    if baseline is None:
        baseline = 0 * inputs

    # scale inputs and compute gradients
    scaled_inputs = [baseline + (float(i+1) / steps) * (inputs - baseline) for i in range(0, steps)]
    grads, _ = predict_and_gradients(scaled_inputs, target_labels, model, loss_fn, optimizer)
    avg_grads = np.average(grads[:-1], axis=0)
    delta_X = (inputs - baseline).detach().squeeze(0).cpu().numpy()
    integrated_grad = delta_X * avg_grads
    return integrated_grad, scaled_inputs

def pred_and_grad(inputs, label_target, model, loss_fn, optimizer):
    '''
    Extracting the grad in the nerual network by Pytorch.

    Args:
        inputs[np.ndarray]: input `X` of dataset.
        target_labels[np.ndarray]: labels of dataset.
        model[nn.Module]: network model.
        loss_fn: used loss function for training.
        optimizer: used optimizer for training.

    Returns:
        integrated_grad[np.ndarray]: the attribution value generated by Integrated Gradient method.
        scaled_input[list(np.ndarray)]: the interpolation from `X` to `X'` .
        
    '''
    grads = []
    for input in inputs:
        input_tensor = input.float().clone().detach().to(device).requires_grad_(True)
        out = model(input_tensor).squeeze()
        label_pred = torch.max(out,-1)[1]
        label_target = torch.tensor(label_target)
        loss = loss_fn(out.to(cpu).unsqueeze(0), label_target.unsqueeze(0))

        #backpropagation
        optimizer.zero_grad()
        loss.backward(retain_graph=True)
        # label_pred.backward(torch.ones_like(label_pred))
        # input_tensor = -torch.log10(input_tensor)
        grad = input_tensor.grad.detach().cpu().numpy()
        grads.append(grad)
    grads = np.array(grads)

    return grads, label_pred
    
def visualize(attributions):
    '''
    Sum every 25 adjacent points. For instance, 200 points' attribution will transform into 8 points.

    Args:
        attributions[np.ndarray]: the original attribution value.

    Returns:
        locsum[np.ndarray]: the new attribution value.
    '''
    attributions = np.abs(attributions)
    locsum = []
    loc = 0
    for i in range(len(attributions)):
        if (i+1) % 25 == 0:
            locsum.append(loc)
            loc = 0
        else:
            loc += attributions[i]
    locsum = np.array(locsum) / np.sum(attributions)
    # cumsum = np.cumsum(attributions) / np.sum(attributions)
    return locsum

